# SGOS Documentation - LLM Reference
# Generated: 2026-01-16 10:36
# Source: https://sgos-infra.sgl.as
# Entries: 14

---
## SGOS Documentation
path: docs/intro.md
url: https://sgos-infra.sgl.as

SGOS (Sonnenglas Operating System): modular API-first internal ERP, AI-native design where APIs serve UIs/LLMs/agents equally.

APPS: Phone (sgos-phone, phone.sgl.as, Live): voicemail processing. Ikhaya (sgos-ikhaya, ikhaya.sgl.as, Live): internal knowledge base. Docflow (sgos-docflow, docflow.sgl.as, Beta): document management (invoices, contracts). Ufudu (sgos-ufudu, ufudu.sgl.as, Beta): warehouse fulfillment pick/pack. Accounting (sgos-accounting, accounting.sgl.as, Alpha): financial transactions, VAT, exports. Inventory (sgos-inventory, inventory.sgl.as, Alpha): stock management. Baobab (sgos-baobab, baobab.sgl.as, Concept): product master, brands, listings. Directory (sgos-directory, directory.sgl.as, Alpha): user directory. Xhosa (sgos-xhosa, xhosa.sgl.as, Concept): order management, CRM, invoicing. Soup (sgos-soup, soup.sgl.as, Concept): task management. Anansi (sgos-anansi, anansi.sgl.as, Concept): AI chatbot. Clock (sgos-clock, clock.sgl.as, Concept): time tracking. MRP (sgos-mrp, mrp.sgl.as, Planned): manufacturing planning.

SERVERS: Hornbill (apps, 100.67.57.25), Toucan (control, 100.102.199.98). Both Netcup VPS Ubuntu 24.04 in Nuremberg, Tailscale mesh, Cloudflare Tunnel external access.

SHARED SERVICES: SGOS Status (sgos-status.sgl.as, Live): health dashboard. Grafana (grafana.sgl.as, Live): Loki logs. GlitchTip (glitchtip.sgl.as, Live): error tracking. PocketID (id.sgl.as, Live): OIDC identity. Sangoma (sangoma.sgl.as, Concept): automated error analysis. Message Bus (Concept): async events.

STACK: Docker Compose source-based deploys via GitHub webhooks, app.json config (Heroku-compatible), SOPS+age secrets, Restic backups to Cloudflare R2, Cloudflare Zero Trust + PocketID auth (Google Workspace or passkeys/WebAuthn, service tokens for API).

NAMING: repo sgos-<name>, container sgos-<name>-app, path /srv/apps/sgos-<name>/, domain <name>.sgl.as.

---
## Architecture Overview
path: docs/architecture.md
url: https://sgos-infra.sgl.as/architecture

SGOS Architecture: Servers: Hornbill (business apps), Toucan (control plane). Apps on Hornbill: Phone (Twilio telephony gateway, Live), Xhosa (order management/ERP core, Live), Docflow (document processing/archive, Live), Accounting (financial records/reporting, Planned), Inventory (stock levels/movements, Planned), Baobab (product/brand master data, Concept), MRP (manufacturing planning, Concept), Ufudu (partner portal backend, Planned), Soup (content API for website, Planned), Anansi (competitor price crawling, Concept), Ikhaya (B2B webshop, Concept), Directory (employee directory, Planned), Clock (time tracking, Concept), Bus (event message broker, Concept), Identity (SSO/authentication, Concept). Apps on Toucan: Sangoma (automated error analysis, Concept), Grafana, Loki, GlitchTip, Beszel. External systems: Twilio→Phone, Dropscan→Docflow, Google Workspace→Directory, sonnenglas.net→Soup, Partner Portal→Ufudu. Integration methods: Twilio→Phone (webhook, incoming calls), Dropscan→Docflow (webhook, scanned documents), Phone→Xhosa (internal API, order creation), Xhosa→Accounting (internal API, invoice generation), Xhosa→Inventory (internal API, stock reservation), Docflow→Accounting (internal API, payables from invoices), Bus→all apps (Redis pub/sub, event distribution), Identity→all apps (OAuth/OIDC, authentication), GlitchTip→Sangoma (API polling, error collection), Sangoma→GitHub (API, PR/issue creation). Order flow: Customer→Phone→Xhosa (creates/updates order)→Inventory (reserves stock)→Accounting (creates invoice)→Docflow (archives PDF, sends email). Document flow: Dropscan→Docflow (webhook)→classifies→if invoice: Accounting (create payable)→Xhosa (link to supplier order); if contract: archive/notify. Data flows: Hornbill→Toucan (logs, errors), Toucan→Hornbill (fixes). App groupings: Customer Touchpoints (Phone, Ikhaya, Ufudu, Soup), Core Business (Xhosa, Docflow, Accounting, Inventory), Manufacturing (MRP, Baobab), Intelligence (Anansi), Internal Tools (Directory, Clock), Platform Services (Bus, Identity, Sangoma).

---
## Apps Overview
path: docs/apps/overview.md
url: https://sgos-infra.sgl.as/apps/overview

SGOS Applications - Business: Phone (sgos-phone, phone.sgl.as, Live): voicemail processing/transcription via Placetel API, connects Anansi, APIs: Private /api/..., Internal /api/int/v1/.... Xhosa (sgos-xhosa, xhosa.sgl.as, Planned): order management, VAT, invoicing, forecasting, CRM, inputs: Website/Stripe/Amazon/Platform Sales/Partner Portal/staff, connects Human in the Soup/Accounting/Ufudu/Inventory/Stripe. Docflow (sgos-docflow, docflow.sgl.as, Planned): incoming documents (mail/contracts/invoices) as structured data, outputs bank files for batch payments, connects Human in the Soup/Accounting. Accounting (sgos-accounting, accounting.sgl.as, Planned): financial single source of truth, inputs: banks/Stripe/manual entries, outputs: VAT/DATEV exports, connects Human in the Soup/Docflow/Xhosa/DATEV. Inventory (sgos-inventory, inventory.sgl.as, Planned): stock movements/quantities/reservations/history, references Baobab products, connects Baobab/Xhosa/Accounting. Baobab (sgos-baobab, baobab.sgl.as, Concept): PIM system, product master data/SKUs/brands/marketplace listings/content/categories/BOM references, connects Inventory/Xhosa/MRP/Ufudu/Website. MRP (sgos-mrp, mrp.sgl.as, Planned): manufacturing/production, flat-file architecture, BOM from Baobab, work orders, barcode scanning, SOPs/Incoming Goods/QC flows, connects Baobab/Inventory/Xhosa/Accounting/Human in the Soup. Ufudu (sgos-ufudu, ufudu.sgl.as, Planned): pick/pack fulfillment, mobile SPA with barcode scanning for Android, connects Human in the Soup/Inventory/Xhosa. Human in the Soup (sgos-soup, soup.sgl.as, Planned): shared todo list for human intervention requests from all systems including LLM agents, Kanban project management UI, connects all systems. Anansi (sgos-anansi, anansi.sgl.as, Planned): AI assistant with vector database, internal/external chatbot, scoped knowledge classification, connects all systems. Ikhaya (sgos-ikhaya, ikhaya.sgl.as, Planned): internal knowledge base/blog via Docusaurus, embeds Anansi chatbot, outputs to Google Chat notifications. Directory (sgos-directory, directory.sgl.as, Planned): user directory API, syncs Google Workspace, apps manage own permissions, connects all modules. Clock (sgos-clock, clock.sgl.as, Concept): time tracking/attendance, Scheduled_Work entries for plan vs actual, clock-in/out, leave scheduling, GPS/IP location enforcement, Nagar IT API for holidays, RRULE-based schedules, payroll exports CSV/PDF, NFC/terminal clock-in optional, connects Directory/Nagar IT API.

Infrastructure: Message Bus (sgos-bus, bus.sgl.as, Planned): Postgres-backed append-only event bus, shared event dictionary: order.created/order.shipped (Xhosa/Ufudu), stock.movement/stock.low (Inventory), invoice.received (Docflow), payment.matched (Accounting), production.completed (MRP), task.created (Human in the Soup). Identity (sgos-id, id.sgl.as, Live): Pocket ID self-hosted OIDC provider, passkeys/WebAuthn, connects Cloudflare Zero Trust/all apps. Sangoma (sgos-sangoma, sangoma.sgl.as, Concept): automated error analysis, monitors GlitchTip, Claude analyzes errors, creates GitHub PRs (high confidence) or issues (low confidence), never auto-applies fixes, notifies via Google Chat, maintains per-app context memory.

External: Website (sonnenglas.net, Cloudflare Pages): Astro.js static with dynamic islands, Stripe checkout, orders to Xhosa, connects Anansi. Partner Portal (partner.sonnenglas.net): reseller portal, orders to Xhosa.

Architecture: All apps behind Tailscale (tailnet: tail5b811.ts.net), expose 80/443 via Cloudflare Tunnel, Cloudflare Zero Trust access control (Google Login/Pocket ID/Service Auth/app-specific API keys with scopes), internal APIs via MagicDNS. All SGOS apps have same API structure: Private /api/..., Internal /api/int/v1/..., no Public APIs.

---
## App Schema (app.json)
path: docs/apps/app-schema.md
url: https://sgos-infra.sgl.as/apps/app-schema

app.json schema for SGOS apps, based on Heroku app.json. Core fields: name (sgos-<name>), description, version (semver), migration (none|safe|breaking), repository (GitHub URL). Migration values: none=no DB changes/rollback safe, safe=additive only/rollback safe, breaking=destructive/needs DB restore. env field documents required environment variables (values in .env). Scripts: postdeploy (runs after deployment for migrations), backup (called by Toucan). SGOS extensions (sgos object): domain (public domain), dependencies (other SGOS apps), backup.output (backup directory), apis (private/internal/public endpoints). Required files per app: app.json, CHANGELOG.md (Keep a Changelog format), .env.sops (encrypted secrets), .sops.yaml (SOPS config), docker-compose.yml. Docker network requirement: apps must join external "sgos" network for centralized proxy/maintenance mode, optional "internal" network for DB isolation. Health check required: Docker healthcheck with test command returning 0 when healthy, interval, timeout, retries, start_period (grace period for migrations). App must expose /health endpoint returning HTTP 200 when ready, verifying DB connectivity and dependencies. Deployment script waits for "healthy" status before exiting maintenance mode. Location: /srv/apps/sgos-<name>/app.json on server and in git repository.

---
## API Strategy
path: docs/apps/api-strategy.md
url: https://sgos-infra.sgl.as/apps/api-strategy

SGOS API Strategy: Agent Native Infrastructure - all operations accessible to both humans and LLM agents via API, no UI-only features. API Types: Private (own frontend, unstable, unversioned), Internal (other SGOS apps, stable, versioned), Public (external consumers, stable, versioned). URL Convention: Private at /api/ with /api/docs and /api/openapi.json; Internal v1 at /api/int/v1/ with /api/int/v1/docs and /api/int/v1/openapi.json; Internal v2 at /api/int/v2/; Public v1 at /api/pub/v1/ with /api/pub/v1/docs and /api/pub/v1/openapi.json. Rules: docs and openapi.json always together, internal/public must be versioned, private unversioned, apps may implement subset of API types. FastAPI implementation: main app with docs_url=None, mount sub-applications (FastAPI instances) at /api, /api/int/v1, /api/pub/v1 with each having own docs_url="/docs" and openapi_url="/openapi.json". OpenAPI as contract: Pydantic schemas define API, FastAPI generates openapi.json, commit to git, test against spec. Versioning: adding optional fields or new endpoints = no version bump; removing fields/endpoints or changing field types = new version (v2) with deprecation period.

---
## Logging Standard
path: docs/apps/logging.md
url: https://sgos-infra.sgl.as/apps/logging

SGOS Logging Standard: Apps log to stdout, collected by Docker → Alloy → Loki (30 days retention) → Grafana. Minimum setup: none, any stdout/stderr auto-collected. Query: `{container="sgos-myapp-app"}`. Recommended: structured JSON for queryable fields. Required JSON fields: ts (ISO 8601 with Z), level (debug/info/warn/error), msg (human-readable). Recommended fields: request_id, user_id, duration_ms. App identification automatic via container label. Log levels: debug (disabled in production), info (normal ops), warn (recoverable issues), error (failures needing attention). FastAPI: JSONFormatter class formats single-line JSON, setup_logging() configures root logger, quiet uvicorn.access and httpx at WARNING. Middleware adds request_id to all logs. Grafana LogQL queries: `{container="sgos-xhosa-app"} | json | level="error"` for errors, `| json | duration_ms > 500` for slow requests, `| json | user_id="email"` for user activity, `| json | order_id=N` for order lookup, `sum(rate({server="hornbill"} | json | level="error" [5m])) by (container)` for error rate. Error tracking via GlitchTip/Sentry SDK (dsn: glitchtip.sgl.as). Don'ts: no sensitive data (passwords, tokens, card numbers), no unnecessary PII, no print(), no logging in tight loops.

---
## Architecture
path: docs/infrastructure/architecture.md
url: https://sgos-infra.sgl.as/infrastructure/architecture

Servers: Toucan (control, 152.53.160.251 public, 100.102.199.98 tailscale, 8 vCPU, 16GB RAM, 1TB NVMe) runs Grafana+Loki, GlitchTip, PocketID, SGOS Status, webhook receiver, backup orchestration, Watchtower; Hornbill (apps, 159.195.68.119 public, 100.67.57.25 tailscale, 12 vCPU, 32GB RAM, 1TB NVMe) runs all sgos-* apps, Alloy log shipper, Cloudflare Tunnel. Both hosted Netcup Nuremberg Germany. SSH via Tailscale only (ssh stefan@toucan, ssh stefan@hornbill). Network: Cloudflare Zero Trust tunnels → *.sgl.as → servers connected via Tailscale mesh. Security: public access Cloudflare Tunnel only, internal Tailscale, UFW denies all except tailscale0 and port 443. Database: each app owns Postgres instance, cross-app analytics via read-only DB synced nightly. Directories: Hornbill /srv/apps/sgos-<name>/ contains app.json, docker-compose.yml, .env, src/, data/, backup/; /srv/proxy/hornbill/ for maintenance mode; /srv/services/alloy/ for logs. Toucan /srv/config/monitoring/ for Grafana/Loki/Alloy config; /srv/services/ contains status/, webhook/, backups/, glitchtip/, sgos-infra/; /srv/backups/ has staging/ and status.json; /srv/proxy/toucan/ for maintenance mode.

---
## Deployment
path: docs/infrastructure/deployment.md
url: https://sgos-infra.sgl.as/infrastructure/deployment

Deployment model: source-based (code on server via git), Docker Compose orchestration, Cloudflare Tunnel external access, GitHub webhooks automation, app.json + docker-compose.yml config, SOPS-encrypted .env.sops secrets. Branch strategy: main = production (auto-deploys on push), feature branches = development (no auto-deploy). Webhook architecture: GitHub push → Toucan webhook:9000 → hooks.json → deploy-*.sh → SSH to target → git pull, docker build, health check. Webhook security: HMAC-SHA256 signature validation, branch filter (refs/heads/main only), repository filter (sonnenglas/* repos). Current hooks: deploy-sgos-infra (sonnenglas/sgos-infra → Toucan, deploy-sgos-infra.sh), deploy-sgos-phone (sonnenglas/sgos-phone → Hornbill, deploy-sgos-phone.sh), deploy-sgos-sangoma (sonnenglas/sgos-sangoma → Toucan, deploy-sgos-sangoma.sh). Maintenance mode: single nginx reverse proxy per server (sgos-proxy) between Cloudflare Tunnel and apps, checks flag file existence → serves maintenance.html or forwards to app. Maintenance flow: touch flag → nginx serves maintenance page → app rebuilds → health check passes → remove flag → nginx resumes. Manual maintenance: touch /srv/proxy/hornbill/flags/<app>.flag (enter), rm flag (exit), touch global.flag (all apps). New app setup: 1) create webhook/scripts/deploy-sgos-<app>.sh (SSH to target, touch PROXY_FLAG, git pull, sops decrypt, docker compose up --build, wait health check, rm flag), 2) add hook to hooks.json with HMAC-SHA256 trigger rule, 3) GitHub webhook: payload URL https://webhook.sgl.as/hooks/deploy-sgos-<app>, content-type application/json, secret = WEBHOOK_SECRET, push event only, 4) add nginx server block to /srv/proxy/hornbill/nginx.conf with resolver 127.0.0.11, maintenance error_page 503, flag checks, 5) restart webhook on Toucan (docker compose down/up), reload proxy on Hornbill (docker exec sgos-proxy nginx -s reload). Manual deployment: SSH to server, touch flag, cd src && git pull, sops -d src/.env.sops > .env, docker compose up -d --build, rm flag. Rollback: git log --oneline, git checkout <commit>, docker compose up -d --build; check app.json migration field first, if breaking restore database backup. Troubleshooting: webhook logs (docker logs -f webhook on Toucan), stuck maintenance (rm flag), 502 after restart (wait 10s DNS cache or nginx -s reload), app not reachable (verify sgos network: docker inspect/network connect). Files: /srv/services/webhook/hooks.json (hook definitions), /srv/services/webhook/scripts/ (deploy scripts), /srv/proxy/*/nginx.conf (proxy config), /srv/proxy/*/flags/ (maintenance flags).

---
## Monitoring
path: docs/infrastructure/monitoring.md
url: https://sgos-infra.sgl.as/infrastructure/monitoring

Monitoring services: SGOS Status (sgos-status.sgl.as, Toucan:3004, real-time health dashboard), Grafana (grafana.sgl.as, log analytics), GlitchTip (glitchtip.sgl.as, Toucan:8000 internal, Sentry-compatible error tracking). Log aggregation: Alloy collects Docker container logs on both servers (labeled server=toucan/server=hornbill), sends to Loki on Toucan:3100 (30-day retention), Grafana queries. LogQL examples: {server="hornbill"}, {container="phone"}, {server="toucan"} |= "error". Status page architecture: status.py on Toucan SSHs to Hornbill, collects docker inspect/git log/app.json, writes apps.json, served via nginx:80 through Cloudflare Tunnel. Status states: healthy (green pulse), unhealthy (red pulse), starting (yellow pulse), not-running (gray). Scheduled jobs: Toucan backup-orchestrator.sh daily 03:00, status.py every minute. Auto-login via Cloudflare Zero Trust: Cf-Access-Authenticated-User-Email header; GlitchTip uses Django REMOTE_USER; Beszel uses TRUSTED_AUTH_HEADER; Dozzle uses DOZZLE_AUTH_PROVIDER=forward-proxy with DOZZLE_AUTH_HEADER_USER/EMAIL. Watchtower auto-updates monitoring containers daily 4 AM (requires label com.centurylinklabs.watchtower.enable=true). Paths: Grafana/Loki/Alloy at Toucan /srv/config/monitoring/, GlitchTip at Toucan /srv/services/glitchtip/, Status at Toucan /srv/services/status/, Alloy at Hornbill /srv/services/alloy/. SSH deploy key: /home/stefan/.ssh/deploy_hornbill. Health check command: docker inspect --format='{{.State.Health.Status}}' sgos-<app>-app.

---
## Backups
path: docs/infrastructure/backups.md
url: https://sgos-infra.sgl.as/infrastructure/backups

Backups: Two-stage architecture - apps create local backups, Toucan orchestrates collection and offsite sync. Flow: Hornbill /srv/apps/*/backups/ (app outputs, 7-day local retention) → rsync pull 3 AM → Toucan /srv/backups/staging/ (latest sync only) → restic encrypted → Cloudflare R2 sonnenglas-backups (7 daily, 4 weekly, 3 monthly retention). Orchestrator: /srv/services/backups/backup-orchestrator.sh runs nightly 3 AM cron on Toucan, SSHs to servers, executes app backup.sh scripts, pulls via rsync, syncs to R2. Files on Toucan: /srv/services/backups/.env (R2 credentials, restic password), /srv/services/backups/backup-orchestrator.sh, /srv/backups/staging/, /srv/backups/status.json (per-app status), /srv/backups/backup.log. Backed up: sgos-phone (PostgreSQL database, voicemail MP3s). Not backed up: Redis (cache), Loki logs (30-day internal retention), Docker images (registry-pulled), source code (GitHub). Operations: check status `cat /srv/backups/status.json`, view log `tail -100 /srv/backups/backup.log`, list R2 snapshots `cd /srv/services/backups && source .env && export AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY RESTIC_REPOSITORY RESTIC_PASSWORD && restic snapshots`, manual backup `/srv/services/backups/backup-orchestrator.sh`. Restore: setup exports (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, RESTIC_REPOSITORY, RESTIC_PASSWORD from .env), `restic snapshots` to list, `restic ls latest` to browse, `restic restore latest --target /tmp/restore --include "sgos-phone"` to restore. Phone DB restore: restic restore to /tmp, scp database.sql to hornbill:/tmp, on Hornbill stop phone container, `docker exec -i sgos-phone-db psql -U postgres -d phone < /tmp/database.sql`, restart. Voicemail restore: `restic restore latest --target /tmp/restore --include "sgos-phone/voicemails"`, scp MP3s to hornbill:/srv/apps/sgos-phone/data/voicemails/. Specific date: use snapshot ID from `restic snapshots`. Verification: `restic check` for integrity, `restic restore latest --target /tmp/verify-test --verify`. Add new app: create backup.sh in app repo, add scripts.backup and sgos.backup.output to app.json, add app to backup-orchestrator.sh on Toucan.

---
## Secrets
path: docs/infrastructure/secrets.md
url: https://sgos-infra.sgl.as/infrastructure/secrets

Secrets encrypted with SOPS + age encryption. File convention: .env.sops (encrypted, committed), .env (decrypted, gitignored), .env.example (template, optional). Config file .sops.yaml at repo root with creation_rules path_regex \.env\.sops$ and age public key age1nh9zuzsmewquyr0xlv7vzzsug0fat6ju5kznxhlpkcrujwtjevyqe6vl5g. Private key location on all machines (Mac, Toucan, Hornbill): ~/.config/sops/age/keys.txt. Environment variable: export SOPS_AGE_KEY_FILE="$HOME/.config/sops/age/keys.txt". Operations: create secret (echo to .env, sops -e -i, rename to .env.sops), edit (sops service/.env.sops opens in $EDITOR), decrypt for deploy (sops --input-type dotenv --output-type dotenv -d .env.sops > .env), view (sops -d .env.sops). Deployment workflow: git pull, decrypt with sops, docker compose up -d. Secret rotation: edit locally with sops, commit/push, on server git pull && sops -d .env.sops > .env && docker compose restart. Security: private key in 1Password and servers, public key safe to commit, same age key works across all repos. Backup: 1Password (primary), server key files. Key loss = unrecoverable secrets.

---
## Cloudflare
path: docs/infrastructure/cloudflare.md
url: https://sgos-infra.sgl.as/infrastructure/cloudflare

Cloudflare managed via Terraform in /cloudflare/. Resources: tunnels (toucan.sgl.as, hornbill), tunnel configs (ingress rules), DNS CNAME records, Access Applications (Zero Trust), Access Policies. Workflow: cd cloudflare && terraform plan && terraform apply. Adding service: 1) ingress rule in tunnel.tf, 2) DNS CNAME in dns.tf, 3) optional Zero Trust in access.tf, 4) terraform apply. Secrets: terraform.tfvars (gitignored) with API token requiring Zone/DNS/Edit, Account/Zero Trust/Edit, Account/Cloudflare Tunnel/Edit. State: terraform.tfstate (local, gitignored, backup required). cloudflared daemon runs on Toucan and Hornbill as systemd service with token-based config: ExecStart=/usr/bin/cloudflared --no-autoupdate tunnel run --token <token>. Ingress rules managed remotely via Terraform cloudflare_zero_trust_tunnel_cloudflared_config, no local config files, changes require terraform apply. Apps use nginx sidecar for maintenance mode. Status check: systemctl status cloudflared, journalctl -u cloudflared -f. Zero Trust auth methods: Google Workspace (primary team login), PocketID OIDC (non-Google users), Service Tokens (API/automation). Access app HCL: cloudflare_zero_trust_access_application with zone_id, name, domain, type=self_hosted, session_duration=24h; cloudflare_zero_trust_access_policy with application_id, zone_id, name, decision=allow, precedence, include gsuite block with identity_provider_id.

---
## Disaster Recovery
path: docs/infrastructure/disaster-recovery.md
url: https://sgos-infra.sgl.as/infrastructure/disaster-recovery

Disaster Recovery scenarios: (1) Hornbill failure impacts all SGOS apps, recovery: provision new server (Ubuntu 24.04 LTS, min 8 vCPU/16GB RAM/500GB NVMe), install Tailscale+Docker, restore SSH keys from Toucan (scp stefan@toucan:/home/stefan/.ssh/deploy_hornbill), create /srv/{infra,services,apps}, restore apps via restic from s3:s3.eu-central-1.amazonaws.com/sgos-backups, clone repos, decrypt secrets (sops -d src/.env.sops > .env), update Cloudflare tunnel IP via terraform apply in /cloudflare, RTO ~2h RPO ~24h. (2) Toucan failure impacts monitoring/backups/webhooks/status, apps continue, recovery: provision server (Ubuntu 24.04 LTS, min 4 vCPU/8GB RAM/200GB NVMe), restore monitoring stack to /srv/services/monitoring, restore backup orchestrator to /srv/services/backups, restore crons (0 3 * * * /srv/services/backups/backup-orchestrator.sh, * * * * * /usr/bin/python3 /srv/services/status/status.py), generate new SSH key (ssh-keygen -t ed25519 -f ~/.ssh/deploy_hornbill), restore cloudflared tunnel with token, restore R2 credentials from 1Password, RTO ~3h. (3) Data corruption: stop app, list snapshots via restic, restore specific snapshot with --include filter, replace data, restore database via docker exec psql. (4) SOPS key loss: retrieve from 1Password (SGOS vault → "age secret key") to ~/.config/sops/age/keys.txt, if unavailable regenerate all secrets and rotate credentials. (5) Cloudflare compromise: revoke API tokens, rotate tunnel secrets, review Zero Trust logs, generate new Terraform token, re-run terraform apply. Not backed up (recreate from docs): server OS config (hornbill-setup.md, toucan-setup.md), Docker networks, Cloudflare tunnels (Terraform), SSH keys, cron jobs, Grafana dashboards. Emergency contacts: Netcup (support@netcup.de), Cloudflare (dashboard), GitHub, 1Password. Quarterly DR tests: backup restore to /tmp, key recovery via 1Password, documentation walkthrough on test VM.

---
## Authentication
path: docs/infrastructure/authentication.md
url: https://sgos-infra.sgl.as/infrastructure/authentication

Authentication: All services behind Cloudflare Zero Trust. Methods: Google Workspace (primary IdP, @sonnenglas.net), PocketID (self-hosted OIDC, passkey auth, https://id.sgl.as), Cloudflare header auto-login (Cf-Access-Authenticated-User-Email), App OIDC. Apps: Dashboard (dashboard.sgl.as, Zero Trust Google, no login), Beszel (beszel.sgl.as, Zero Trust Google, auto-login via TRUSTED_AUTH_HEADER=Cf-Access-Authenticated-User-Email), Dozzle (dozzle.sgl.as, Zero Trust Google, auto-login via DOZZLE_AUTH_PROVIDER=forward-proxy, DOZZLE_AUTH_HEADER_USER/EMAIL/NAME=Cf-Access-Authenticated-User-Email), SGOS Infra Docs (sgos-infra.sgl.as, Zero Trust Google, static), GlitchTip (glitchtip.sgl.as, Zero Trust Google, requires separate OIDC login via PocketID/Google, configure in Django Admin /admin/socialaccount/socialapp/ with provider OpenID Connect, settings {"server_url": "https://id.sgl.as"}), PocketID (id.sgl.as, no Zero Trust, passkey login, is the IdP, env: APP_URL=https://id.sgl.as, TRUST_PROXY=true, ENCRYPTION_KEY), Phone (phone.sgl.as, no Zero Trust, public webhook). PocketID OIDC endpoints: authorize https://id.sgl.as/authorize, token https://id.sgl.as/api/oidc/token, JWKS https://id.sgl.as/.well-known/jwks.json. Cloudflare Zero Trust PocketID integration: Settings → Authentication → Add OpenID Connect login method.
