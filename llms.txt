# SGOS Documentation - LLM Reference
# Generated: 2026-01-15 14:28
# Source: https://sgos-infra.sgl.as
# Entries: 12

---
## API Strategy
path: docs/apps/api-strategy.md
url: https://sgos-infra.sgl.as/apps/api-strategy

API types: Private (own frontend, unstable, /api/), Internal (other SGOS apps, stable versioned, /api/int/v{n}/), Public (external consumers, stable versioned, /api/pub/v{n}/). URL structure: each API path contains /docs (Swagger UI) and /openapi.json (spec). Private API unversioned; internal/public must be versioned (v1, v2). FastAPI implementation: main app with docs_url=None, openapi_url=None; mount sub-applications at /api, /api/int/v1, /api/pub/v1 with docs_url="/docs", openapi_url="/openapi.json" for isolated documentation. OpenAPI as contract: Pydantic schemas → FastAPI generates spec → commit openapi.json to git → test against spec. Versioning: no bump for adding optional fields or new endpoints; new version required for removing fields/endpoints or changing field types; breaking changes need deprecation period.

---
## App Schema (app.json)
path: docs/apps/app-schema.md
url: https://sgos-infra.sgl.as/apps/app-schema

app.json schema: Core fields: name (sgos-<name>), description, version (semver), migration (none|safe|breaking), repository (GitHub URL). Migration values: none (no DB changes, rollback safe), safe (additive only, rollback safe), breaking (destructive, requires DB restore). env section documents required vars (actual values in .env). Scripts: postdeploy (migrations), backup (called by Toucan). SGOS extensions (sgos object): domain, dependencies, backup.output, apis (private/internal/public). Required files per app: app.json, CHANGELOG.md (Keep a Changelog format), .env.sops (encrypted), .sops.yaml (SOPS config), docker-compose.yml. Docker network requirement: apps must join external "sgos" network for centralized proxy/maintenance mode; optional "internal" network for DB isolation. Health check required: deployment script waits for Docker "healthy" status before exiting maintenance mode; params: test (command returning 0), interval, timeout, retries, start_period (grace period for migrations). Apps must expose /health endpoint returning HTTP 200, verifying DB and dependencies. Location: /srv/apps/sgos-<name>/app.json on server and in git repo.

---
## Apps Overview
path: docs/apps/overview.md
url: https://sgos-infra.sgl.as/apps/overview

SGOS Apps on sgl.as domain. Business Apps: Phone (sgos-phone, phone.sgl.as, Live) - voicemail/transcription via Placetel→Anansi, Private+Internal APIs. Xhosa (sgos-xhosa, xhosa.sgl.as, Planned) - orders/VAT/invoicing/CRM, inputs: Website/Stripe/Amazon/Partner Portal, connects: Human in Soup/Accounting/Ufudu/Inventory/Stripe. Docflow (sgos-docflow, docflow.sgl.as, Planned) - incoming documents/mail/contracts/invoices, outputs: UI/API/Bank Files, connects: Human in Soup/Accounting. Accounting (sgos-accounting, accounting.sgl.as, Planned) - financial transactions/bank feeds, outputs: VAT/DATEV exports, connects: Human in Soup/Docflow/Xhosa/DATEV. Inventory (sgos-inventory, inventory.sgl.as, Planned) - stock movements/quantities/reservations, connects: Baobab/Xhosa/Accounting. Baobab (sgos-baobab, baobab.sgl.as, Concept) - PIM/product master data/SKUs/brands/marketplace listings/BOM references, connects: Inventory/Xhosa/MRP/Ufudu/Website. MRP (sgos-mrp, mrp.sgl.as, Planned) - manufacturing/BOM/work orders/barcode scanning/SOPs, connects: Baobab/Inventory/Xhosa/Accounting/Human in Soup. Ufudu (sgos-ufudu, ufudu.sgl.as, Planned) - pick/pack fulfillment, mobile SPA with barcode scanning on Android, connects: Human in Soup/Inventory/Xhosa. Human in Soup (sgos-soup, soup.sgl.as, Planned) - shared todo list for human intervention requests from all systems, Kanban UI, connects: all systems. Anansi (sgos-anansi, anansi.sgl.as, Planned) - AI assistant with vector DB, internal/external chatbot, knowledge scoped public→internal, connects: all systems. Ikhaya (sgos-ikhaya, ikhaya.sgl.as, Planned) - internal knowledge base/blog (Docusaurus), embeds Anansi chatbot, connects: Anansi. Directory (sgos-directory, directory.sgl.as, Planned) - user directory API, syncs Google Workspace, connects: all modules. Clock (sgos-clock, clock.sgl.as, Concept) - time tracking/attendance/leave/payroll exports, uses Scheduled_Work entries, GPS/IP enforcement, RRULE schedules, Nagar IT API for holidays, connects: Directory. Infrastructure: Message Bus (sgos-bus, bus.sgl.as, Planned) - Postgres-backed event bus, events: order.created/order.shipped/stock.movement/stock.low/invoice.received/payment.matched/production.completed/task.created. Identity (sgos-id, id.sgl.as, Live) - Pocket ID, OIDC provider, passkeys/WebAuthn, connects: Cloudflare Zero Trust. Sangoma (sgos-sangoma, sangoma.sgl.as, Concept) - automated error analysis, fetches GlitchTip errors, Claude analyzes, creates GitHub PRs/issues (never auto-applies), Google Chat notifications. External: Website (sonnenglas.net, Cloudflare Pages) - Astro.js, Stripe checkout→Xhosa, connects: Anansi. Partner Portal (partner.sonnenglas.net) - reseller orders→Xhosa. All apps: Private API /api/..., Internal API /api/int/v1/..., no Public APIs except website. Architecture: Tailnet tail5b811.ts.net, all servers on Tailscale, Cloudflare Tunnel for external access, Zero Trust auth via Google Login/Pocket ID/Service Auth/API keys with scopes.

---
## Architecture
path: docs/infrastructure/architecture.md
url: https://sgos-infra.sgl.as/infrastructure/architecture

Servers hosted at Netcup, Nuremberg, Germany. Toucan (Control): public IP 152.53.160.251, Tailscale 100.102.199.98, 8 vCPU, 16GB RAM, 1TB NVMe; runs Grafana+Loki, GlitchTip, PocketID, SGOS Status, webhook receiver, backup orchestration, Watchtower. Hornbill (Apps): public IP 159.195.68.119, Tailscale 100.67.57.25, 12 vCPU, 32GB RAM, 1TB NVMe; runs all sgos-* apps, Alloy (log shipping to Toucan), Cloudflare Tunnel. SSH via Tailscale only: stefan@toucan, stefan@hornbill. Network: Cloudflare Zero Trust tunnels → *.sgl.as public services → Toucan/Hornbill connected via Tailscale mesh. Security: no direct public access (Cloudflare Tunnel only), Tailscale internal, public SSH disabled, UFW denies all except tailscale0 and port 443. Database: each app gets own Postgres instance; cross-app analytics via read-only DB synced nightly. Hornbill directories: /srv/apps/sgos-<name>/ contains app.json, docker-compose.yml, .env, src/, data/, backup/; /srv/proxy/hornbill/ for maintenance mode; /srv/services/alloy/ for log shipping. Toucan directories: /srv/config/monitoring/ for Grafana/Loki/Alloy; /srv/services/ contains status/, webhook/, backups/, glitchtip/, sgos-infra/; /srv/backups/ with staging/ and status.json; /srv/proxy/toucan/ for maintenance mode.

---
## Authentication
path: docs/infrastructure/authentication.md
url: https://sgos-infra.sgl.as/infrastructure/authentication

Authentication: All services behind Cloudflare Zero Trust. Methods: Google Workspace (primary IdP, @sonnenglas.net), PocketID (self-hosted OIDC, passkey auth, https://id.sgl.as), Cloudflare headers (Cf-Access-Authenticated-User-Email for auto-login), App OIDC. Auto-login apps: Beszel (TRUSTED_AUTH_HEADER=Cf-Access-Authenticated-User-Email), Dozzle (DOZZLE_AUTH_PROVIDER=forward-proxy, DOZZLE_AUTH_HEADER_USER/EMAIL/NAME=Cf-Access-Authenticated-User-Email). Separate app login: GlitchTip (OIDC via PocketID or Google, configure in Django Admin /admin/socialaccount/socialapp/ with settings {"server_url": "https://id.sgl.as"}). No Zero Trust: PocketID (is the IdP), Phone (public webhook). Static apps (no accounts): Dashboard, SGOS Infra Docs. PocketID config: APP_URL=https://id.sgl.as, TRUST_PROXY=true, ENCRYPTION_KEY required. PocketID OIDC endpoints: authorize=/authorize, token=/api/oidc/token, JWKS=/.well-known/jwks.json. Add PocketID to Zero Trust: Settings → Authentication → Add OpenID Connect login method.

---
## Backups
path: docs/infrastructure/backups.md
url: https://sgos-infra.sgl.as/infrastructure/backups

Backups: Two-stage architecture - apps generate local backups, Toucan collects via rsync at 3 AM, restic syncs encrypted to Cloudflare R2 (sonnenglas-backups). Flow: Hornbill /srv/apps/*/backups/ (7-day local retention) → rsync pull → Toucan /srv/backups/staging/ (latest only) → restic → R2 (7 daily, 4 weekly, 3 monthly). Orchestrator: /srv/services/backups/backup-orchestrator.sh runs nightly cron, SSHs to servers, executes app backup.sh scripts, pulls outputs via rsync. Files on Toucan: /srv/services/backups/.env (R2 creds, restic password), /srv/backups/staging/ (collected backups), /srv/backups/status.json (per-app status), /srv/backups/backup.log. Currently backed up: sgos-phone (PostgreSQL database, voicemail MP3s). Not backed up: Redis (cache), Loki logs (30-day internal retention), Docker images (registry-pulled), source code (GitHub). Operations: Check status `cat /srv/backups/status.json`, view log `tail -100 /srv/backups/backup.log`, list R2 snapshots `cd /srv/services/backups && source .env && export AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY RESTIC_REPOSITORY RESTIC_PASSWORD && restic snapshots`, manual trigger `/srv/services/backups/backup-orchestrator.sh`. Restore: Setup exports from .env, `restic snapshots` lists available, `restic ls latest` browses contents, `restic restore latest --target /tmp/restore --include "sgos-phone"` restores. Phone DB restore: Stop app `docker compose stop phone`, restore SQL `docker exec -i sgos-phone-db psql -U postgres -d phone < /tmp/database.sql`, restart `docker compose up -d phone`. Verify: `restic check` for integrity, `restic restore latest --target /tmp/verify-test --verify`. Add new app: Create backup.sh in app repo, add scripts.backup and sgos.backup.output to app.json, add app to backup-orchestrator.sh on Toucan.

---
## Cloudflare
path: docs/infrastructure/cloudflare.md
url: https://sgos-infra.sgl.as/infrastructure/cloudflare

Cloudflare managed via Terraform in /cloudflare/. Resources: tunnels (toucan.sgl.as, hornbill), tunnel configs (ingress rules), DNS CNAMEs pointing to tunnels, Access Applications (Zero Trust protected apps), Access Policies. Workflow: cd cloudflare && terraform plan && terraform apply. Adding service: add ingress rule (tunnel.tf), add DNS CNAME (dns.tf), optionally add Zero Trust (access.tf), terraform apply. Secrets: terraform.tfvars (gitignored) contains API token requiring Zone/DNS/Edit, Account/Zero Trust/Edit, Account/Cloudflare Tunnel/Edit permissions. State: terraform.tfstate stored locally (gitignored, needs backup). cloudflared daemon runs on both Toucan and Hornbill as systemd service with token-based config: ExecStart=/usr/bin/cloudflared --no-autoupdate tunnel run --token <token>. Token auth → Cloudflare returns config → cloudflared routes traffic. Ingress rules managed remotely via cloudflare_zero_trust_tunnel_cloudflared_config, no local config files, changes require terraform apply. Apps use nginx sidecar for maintenance mode (no quick local cloudflared switching). Status: systemctl status cloudflared, journalctl -u cloudflared -f. Zero Trust Access Policies protect all services. Auth methods: Google Workspace (primary team login), PocketID/OIDC (non-Google users), Service Tokens (API/automation). Adding protected app: cloudflare_zero_trust_access_application with zone_id, name, domain, type=self_hosted, session_duration; cloudflare_zero_trust_access_policy with application_id, zone_id, name, decision=allow, precedence, include block with gsuite identity_provider_id.

---
## Deployment
path: docs/infrastructure/deployment.md
url: https://sgos-infra.sgl.as/infrastructure/deployment

Deployment model: source-based (git on server), Docker Compose orchestration, Cloudflare Tunnel external access, GitHub webhooks automation. Branch strategy: main=production (auto-deploy on push), feature branches=no auto-deploy. Webhook architecture: GitHub push → Toucan webhook:9000 (HMAC-SHA256 validated) → hooks.json → deploy-*.sh → SSH to target → git pull, docker build, health check. Security: HMAC-SHA256 signature validation, branch filter (refs/heads/main only), repository filter (sonnenglas/* repos). Current hooks: deploy-sgos-infra (sonnenglas/sgos-infra → Toucan, deploy-sgos-infra.sh), deploy-sgos-phone (sonnenglas/sgos-phone → Hornbill, deploy-sgos-phone.sh), deploy-sgos-sangoma (sonnenglas/sgos-sangoma → Toucan, deploy-sgos-sangoma.sh). Maintenance mode: single nginx proxy (sgos-proxy) per server between Cloudflare Tunnel and apps, checks flag file → exists=maintenance.html, no=forward to app. Flow: touch flag → nginx serves maintenance page → app rebuilds → health check passes → remove flag → nginx resumes. Manual maintenance: touch/rm /srv/proxy/hornbill/flags/<app>.flag, global: /srv/proxy/hornbill/flags/global.flag. New app setup: (1) create webhook/scripts/deploy-sgos-<app>.sh with SSH to target, maintenance flag touch, git pull --ff-only, sops decrypt to .env, docker compose up -d --build, health check loop (120s timeout), flag removal; (2) add hook to hooks.json with id deploy-sgos-<app>, execute-command /scripts/deploy-sgos-<app>.sh, HMAC-SHA256 trigger rule using WEBHOOK_SECRET env var, branch=refs/heads/main, repo=sonnenglas/sgos-<app>; (3) GitHub webhook: payload URL https://webhook.sgl.as/hooks/deploy-sgos-<app>, content-type application/json, secret=WEBHOOK_SECRET, push event only; (4) add nginx server block to /srv/proxy/hornbill/nginx.conf with resolver 127.0.0.11, upstream set, maintenance error_page 503, flag file checks; (5) restart webhook on Toucan (docker compose down/up), reload proxy on Hornbill (docker exec sgos-proxy nginx -s reload). Manual deployment: SSH to server, touch maintenance flag, cd src && git pull, sops -d src/.env.sops > .env, docker compose up -d --build, rm flag. Rollback: git checkout <commit>, docker compose up -d --build; check app.json migration field first, if breaking=restore database backup. Troubleshooting: webhook logs via docker logs -f webhook on Toucan; stuck maintenance=rm flag file; 502 after restart=wait 10s or nginx -s reload; app not reachable=verify on sgos network (docker network connect sgos <container>). Key files: /srv/services/webhook/hooks.json (hook definitions), /srv/services/webhook/scripts/ (deploy scripts), /srv/proxy/*/nginx.conf (proxy config), /srv/proxy/*/flags/ (maintenance flags).

---
## Disaster Recovery
path: docs/infrastructure/disaster-recovery.md
url: https://sgos-infra.sgl.as/infrastructure/disaster-recovery

Disaster Recovery Scenarios: (1) Hornbill failure (app server) - provision Ubuntu 24.04 LTS min 8 vCPU/16GB RAM/500GB NVMe, install Tailscale + Docker, restore SSH keys from Toucan (scp stefan@toucan:/home/stefan/.ssh/deploy_hornbill), create dirs /srv/{infra,services,apps}, restore apps from restic s3:s3.eu-central-1.amazonaws.com/sgos-backups, clone repos, decrypt secrets (sops -d src/.env.sops > .env), update Cloudflare tunnel IP via terraform apply in /cloudflare, RTO ~2h RPO ~24h. (2) Toucan failure (control server) - provision Ubuntu 24.04 LTS min 4 vCPU/8GB RAM/200GB NVMe, apps on Hornbill continue running, restore monitoring stack from git to /srv/services/monitoring, restore backup orchestrator to /srv/services/backups, restore crons (0 3 * * * backup-orchestrator.sh, * * * * * status.py), generate new SSH key for Hornbill (ed25519 deploy_hornbill), restore Cloudflare tunnel via token from dashboard, restore R2 creds from 1Password, RTO ~3h. (3) Data corruption single app - stop app, list snapshots via restic, restore specific snapshot with --include, replace data via scp, restore db (docker exec -i sgos-<app>-db psql -U postgres < backup/database.sql). (4) SOPS key loss - retrieve from 1Password "SGOS vault → age secret key" to ~/.config/sops/age/keys.txt, if unavailable regenerate all secrets and rotate all API keys/passwords. (5) Cloudflare compromise - revoke all API tokens, rotate tunnel secrets, review Zero Trust logs, generate new Terraform token, re-run terraform apply. Not backed up (recreate from docs): server OS config (hornbill-setup.md, toucan-setup.md), Docker networks (auto-created), Cloudflare tunnels (Terraform + dashboard), SSH keys (generate new), cron jobs (in setup files), Grafana dashboards (manual). Emergency contacts: Netcup support@netcup.de, Cloudflare dashboard, GitHub account, 1Password recovery. Quarterly DR tests: backup restore to /tmp, key recovery via 1Password, documentation walkthrough on test VM.

---
## Monitoring
path: docs/infrastructure/monitoring.md
url: https://sgos-infra.sgl.as/infrastructure/monitoring

Monitoring services: SGOS Status (sgos-status.sgl.as, Toucan:3004, real-time health dashboard), Grafana (grafana.sgl.as, log analytics), GlitchTip (glitchtip.sgl.as, Toucan:8000, internal http://toucan:8000, Sentry-compatible error tracking). Log aggregation: Alloy collects Docker logs on both servers (labeled server=toucan/server=hornbill), sends to Loki (Toucan:3100, 30-day retention), queryable via Grafana. LogQL examples: {server="hornbill"}, {container="phone"}, {server="toucan"} |= "error". Status page features: health indicators (healthy=green pulse, unhealthy=red pulse, starting=yellow pulse, not-running=gray), app versions, deployment times, GitHub commit links, cron job viewer, 60s auto-refresh. Status architecture: status.py SSHes to Hornbill, runs docker inspect/git log/reads app.json, writes apps.json, nginx serves via Cloudflare Tunnel. Scheduled jobs: Toucan daily 03:00 backup-orchestrator.sh, Toucan every minute status.py. Paths: Grafana/Loki/Alloy /srv/config/monitoring/ (Toucan), GlitchTip /srv/services/glitchtip/ (Toucan), Status /srv/services/status/ (Toucan), Alloy /srv/services/alloy/ (Hornbill). Auto-login via Cf-Access-Authenticated-User-Email header: GlitchTip uses Django REMOTE_USER, Beszel env TRUSTED_AUTH_HEADER, Dozzle env DOZZLE_AUTH_PROVIDER=forward-proxy + DOZZLE_AUTH_HEADER_USER/EMAIL. Watchtower auto-updates daily 4AM for containers with label com.centurylinklabs.watchtower.enable=true. Commands: Grafana stack cd /srv/config/monitoring && docker compose up -d/logs -f/restart; Status cd /srv/services/status && docker compose ps/restart, python3 status.py, cat apps.json | jq; GlitchTip cd /srv/services/glitchtip; Hornbill Alloy ssh stefan@hornbill && cd /srv/services/alloy. Troubleshooting: status not updating check crontab -l, run python3 /srv/services/status/status.py, verify SSH key /home/stefan/.ssh/deploy_hornbill; logs missing verify docker ps | grep alloy, curl -s http://100.102.199.98:3100/ready, docker logs alloy; wrong state check Docker healthcheck, docker inspect --format='{{.State.Health.Status}}' sgos-<app>-app.

---
## Secrets
path: docs/infrastructure/secrets.md
url: https://sgos-infra.sgl.as/infrastructure/secrets

Secrets management uses SOPS with age encryption. Flow: Mac/CI (has .sops.yaml public key + age private key, can encrypt/decrypt) → Git repo (*.env.sops encrypted files, safe to commit) → Toucan/Hornbill servers (age private key + sops, decrypts at deploy time). Files: .env.sops (encrypted, committed), .env (decrypted, gitignored), .env.example (template, optional). Config: .sops.yaml at repo root with creation_rules path_regex \.env\.sops$ and age public key age1nh9zuzsmewquyr0xlv7vzzsug0fat6ju5kznxhlpkcrujwtjevyqe6vl5g. Key location on all machines (Mac, Toucan, Hornbill): ~/.config/sops/age/keys.txt. Shell profile: export SOPS_AGE_KEY_FILE="$HOME/.config/sops/age/keys.txt". Commands: create encrypted secret (echo to .env → sops -e -i .env → mv to .env.sops), edit (sops service/.env.sops opens in $EDITOR), decrypt for deployment (sops --input-type dotenv --output-type dotenv -d service/.env.sops > service/.env), view (sops -d service/.env.sops). Deployment: git pull → sops decrypt → docker compose up -d. Secret rotation: edit locally with sops → commit/push → on server git pull && sops -d .env.sops > .env && docker compose restart. Security: private key in 1Password and deployed manually to servers, public key safe in .sops.yaml, .env files gitignored, same age key works across all repos. Backup: 1Password (primary) + each server at ~/.config/sops/age/keys.txt; if key lost, encrypted secrets unrecoverable.

---
## SGOS Documentation
path: docs/intro.md
url: https://sgos-infra.sgl.as

SGOS (Sonnenglas Operating System): modular in-house ERP, API-first, AI-native design. Apps: Phone (sgos-phone, phone.sgl.as, Live) voicemail processing; Ikhaya (sgos-ikhaya, ikhaya.sgl.as, Live) internal knowledge base; Docflow (sgos-docflow, docflow.sgl.as, Beta) document management; Ufudu (sgos-ufudu, ufudu.sgl.as, Beta) warehouse fulfillment pick/pack; Accounting (sgos-accounting, accounting.sgl.as, Alpha) financial transactions/VAT/exports; Inventory (sgos-inventory, inventory.sgl.as, Alpha) stock management; Baobab (sgos-baobab, baobab.sgl.as, Concept) product master/brands/listings; Directory (sgos-directory, directory.sgl.as, Alpha) user directory; Xhosa (sgos-xhosa, xhosa.sgl.as, Concept) order management/CRM/invoicing; Soup (sgos-soup, soup.sgl.as, Concept) task management; Anansi (sgos-anansi, anansi.sgl.as, Concept) AI chatbot/assistant; Clock (sgos-clock, clock.sgl.as, Concept) time tracking; MRP (sgos-mrp, mrp.sgl.as, Planned) manufacturing planning. Servers: Hornbill (apps, 100.67.57.25), Toucan (control, 100.102.199.98), both Netcup VPS Nuremberg Ubuntu 24.04. Shared services: SGOS Status (sgos-status.sgl.as, Live) health dashboard; Grafana (grafana.sgl.as, Live) Loki log aggregation; GlitchTip (glitchtip.sgl.as, Live) error tracking; PocketID (id.sgl.as, Live) OIDC identity provider; Sangoma (sangoma.sgl.as, Concept) automated error analysis; Message Bus (Concept) async event bus. Stack: Tailscale mesh + Cloudflare Tunnel networking, Docker Compose source-based deployments via GitHub webhooks, app.json config (Heroku-compatible), SOPS+age secrets, Restic to Cloudflare R2 backups, Cloudflare Zero Trust + PocketID auth (Google Workspace or passkeys/WebAuthn or service tokens). Naming: repo sgos-<name>, container sgos-<name>-app, dir /srv/apps/sgos-<name>/, domain <name>.sgl.as.
