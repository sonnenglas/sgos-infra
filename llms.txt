# SGOS Documentation - LLM Reference
# Generated: 2026-01-15 14:50
# Source: https://sgos-infra.sgl.as
# Entries: 12

---
## SGOS Documentation
path: docs/intro.md
url: https://sgos-infra.sgl.as

SGOS (Sonnenglas Operating System): internal modular ERP, API-first, AI-native platform for Sonnenglas. Apps: Phone (sgos-phone, phone.sgl.as, Live) voicemail processing; Ikhaya (sgos-ikhaya, ikhaya.sgl.as, Live) knowledge base; Docflow (sgos-docflow, docflow.sgl.as, Beta) document management; Ufudu (sgos-ufudu, ufudu.sgl.as, Beta) warehouse fulfillment; Accounting (sgos-accounting, accounting.sgl.as, Alpha) financial transactions/VAT; Inventory (sgos-inventory, inventory.sgl.as, Alpha) stock management; Baobab (sgos-baobab, baobab.sgl.as, Concept) product master/PIM; Directory (sgos-directory, directory.sgl.as, Alpha) user directory; Xhosa (sgos-xhosa, xhosa.sgl.as, Concept) order management/CRM; Soup (sgos-soup, soup.sgl.as, Concept) task management; Anansi (sgos-anansi, anansi.sgl.as, Concept) AI assistant; Clock (sgos-clock, clock.sgl.as, Concept) time tracking; MRP (sgos-mrp, mrp.sgl.as, Planned) manufacturing planning. Servers: Hornbill (apps, 100.67.57.25), Toucan (control, 100.102.199.98), both Netcup VPS Nuremberg Ubuntu 24.04. Shared services: SGOS Status (sgos-status.sgl.as, Live) health dashboard; Grafana (grafana.sgl.as, Live) Loki logs; GlitchTip (glitchtip.sgl.as, Live) error tracking; PocketID (id.sgl.as, Live) OIDC identity; Sangoma (sangoma.sgl.as, Concept) automated error analysis; Message Bus (Concept) event bus. Stack: Tailscale mesh + Cloudflare Tunnel networking, Docker Compose source-based deploys via GitHub webhooks, app.json config (Heroku-compatible), SOPS+age secrets, Restic to Cloudflare R2 backups, Cloudflare Zero Trust + PocketID auth (Google Workspace or passkeys). Naming: repo sgos-<name>, container sgos-<name>-app, dir /srv/apps/sgos-<name>/, domain <name>.sgl.as.

---
## Apps Overview
path: docs/apps/overview.md
url: https://sgos-infra.sgl.as/apps/overview

SGOS Applications - Business: Phone (sgos-phone, phone.sgl.as, Live): voicemail processing/transcription via Placetel API, connects Anansi, APIs: Private /api/..., Internal /api/int/v1/.... Xhosa (sgos-xhosa, xhosa.sgl.as, Planned): order management, VAT, invoicing, forecasting, CRM, inputs: Website/Stripe/Amazon/Platform Sales/Partner Portal/staff, connects Human in the Soup/Accounting/Ufudu/Inventory/Stripe. Docflow (sgos-docflow, docflow.sgl.as, Planned): incoming documents (mail/contracts/invoices) as structured data, outputs bank files for batch payments, connects Human in the Soup/Accounting. Accounting (sgos-accounting, accounting.sgl.as, Planned): financial single source of truth, inputs: banks/Stripe/manual entries, outputs: VAT/DATEV exports, connects Human in the Soup/Docflow/Xhosa/DATEV. Inventory (sgos-inventory, inventory.sgl.as, Planned): stock movements/quantities/reservations/history, references Baobab products, connects Baobab/Xhosa/Accounting. Baobab (sgos-baobab, baobab.sgl.as, Concept): PIM system, product master data/SKUs/brands/marketplace listings/content/categories/BOM references, connects Inventory/Xhosa/MRP/Ufudu/Website. MRP (sgos-mrp, mrp.sgl.as, Planned): manufacturing/production, flat-file architecture, BOM from Baobab, work orders, barcode scanning, SOPs/Incoming Goods/QC flows, connects Baobab/Inventory/Xhosa/Accounting/Human in the Soup. Ufudu (sgos-ufudu, ufudu.sgl.as, Planned): pick/pack fulfillment, mobile SPA with barcode scanning for Android, connects Human in the Soup/Inventory/Xhosa. Human in the Soup (sgos-soup, soup.sgl.as, Planned): shared todo list for human intervention requests from all systems including LLM agents, Kanban project management UI, connects all systems. Anansi (sgos-anansi, anansi.sgl.as, Planned): AI assistant with vector database, internal/external chatbot, scoped knowledge classification, connects all systems. Ikhaya (sgos-ikhaya, ikhaya.sgl.as, Planned): internal knowledge base/blog via Docusaurus, embeds Anansi chatbot, outputs to Google Chat notifications. Directory (sgos-directory, directory.sgl.as, Planned): user directory API, syncs Google Workspace, apps manage own permissions, connects all modules. Clock (sgos-clock, clock.sgl.as, Concept): time tracking/attendance, Scheduled_Work entries for plan vs actual, clock-in/out, leave scheduling, GPS/IP location enforcement, Nagar IT API for holidays, RRULE-based schedules, payroll exports CSV/PDF, NFC/terminal clock-in optional, connects Directory/Nagar IT API.

Infrastructure: Message Bus (sgos-bus, bus.sgl.as, Planned): Postgres-backed append-only event bus, shared event dictionary: order.created/order.shipped (Xhosa/Ufudu), stock.movement/stock.low (Inventory), invoice.received (Docflow), payment.matched (Accounting), production.completed (MRP), task.created (Human in the Soup). Identity (sgos-id, id.sgl.as, Live): Pocket ID self-hosted OIDC provider, passkeys/WebAuthn, connects Cloudflare Zero Trust/all apps. Sangoma (sgos-sangoma, sangoma.sgl.as, Concept): automated error analysis, monitors GlitchTip, Claude analyzes errors, creates GitHub PRs (high confidence) or issues (low confidence), never auto-applies fixes, notifies via Google Chat, maintains per-app context memory.

External: Website (sonnenglas.net, Cloudflare Pages): Astro.js static with dynamic islands, Stripe checkout, orders to Xhosa, connects Anansi. Partner Portal (partner.sonnenglas.net): reseller portal, orders to Xhosa.

Architecture: All apps behind Tailscale (tailnet: tail5b811.ts.net), expose 80/443 via Cloudflare Tunnel, Cloudflare Zero Trust access control (Google Login/Pocket ID/Service Auth/app-specific API keys with scopes), internal APIs via MagicDNS. All SGOS apps have same API structure: Private /api/..., Internal /api/int/v1/..., no Public APIs.

---
## App Schema (app.json)
path: docs/apps/app-schema.md
url: https://sgos-infra.sgl.as/apps/app-schema

app.json schema for SGOS apps, based on Heroku app.json. Core fields: name (sgos-<name>), description, version (semver), migration (none|safe|breaking), repository (GitHub URL). Migration values: none=no DB changes/rollback safe, safe=additive only/rollback safe, breaking=destructive/needs DB restore. env field documents required environment variables (values in .env). Scripts: postdeploy (runs after deployment for migrations), backup (called by Toucan). SGOS extensions (sgos object): domain (public domain), dependencies (other SGOS apps), backup.output (backup directory), apis (private/internal/public endpoints). Required files per app: app.json, CHANGELOG.md (Keep a Changelog format), .env.sops (encrypted secrets), .sops.yaml (SOPS config), docker-compose.yml. Docker network requirement: apps must join external "sgos" network for centralized proxy/maintenance mode, optional "internal" network for DB isolation. Health check required: Docker healthcheck with test command returning 0 when healthy, interval, timeout, retries, start_period (grace period for migrations). App must expose /health endpoint returning HTTP 200 when ready, verifying DB connectivity and dependencies. Deployment script waits for "healthy" status before exiting maintenance mode. Location: /srv/apps/sgos-<name>/app.json on server and in git repository.

---
## API Strategy
path: docs/apps/api-strategy.md
url: https://sgos-infra.sgl.as/apps/api-strategy

API Types: Private (own frontend, unstable, unversioned, /api/), Internal (other SGOS apps, stable versioned, /api/int/v1/), Public (external consumers, stable versioned, /api/pub/v1/). URL structure: each API type has docs at /docs and spec at /openapi.json relative to base path. Private: /api/docs, /api/openapi.json. Internal v1: /api/int/v1/docs, /api/int/v1/openapi.json. Internal v2: /api/int/v2/docs, /api/int/v2/openapi.json. Public v1: /api/pub/v1/docs, /api/pub/v1/openapi.json. Rules: docs and spec must coexist, internal/public must be versioned, private unversioned, apps may implement subset of API types. FastAPI implementation: disable root docs (docs_url=None, openapi_url=None), mount sub-applications at paths with relative docs_url="/docs" and openapi_url="/openapi.json". OpenAPI contract: Pydantic schemas define API, FastAPI generates spec, commit openapi.json to git, test against spec. Versioning: no bump for adding optional fields or new endpoints, new version (v2) required for removing fields/endpoints or changing field types, breaking changes require deprecation period.

---
## Architecture
path: docs/infrastructure/architecture.md
url: https://sgos-infra.sgl.as/infrastructure/architecture

Servers: Toucan (control, 152.53.160.251 public, 100.102.199.98 tailscale, 8 vCPU, 16GB RAM, 1TB NVMe) runs Grafana+Loki, GlitchTip, PocketID, SGOS Status, webhook receiver, backup orchestration, Watchtower; Hornbill (apps, 159.195.68.119 public, 100.67.57.25 tailscale, 12 vCPU, 32GB RAM, 1TB NVMe) runs all sgos-* apps, Alloy log shipper, Cloudflare Tunnel. Both hosted Netcup Nuremberg Germany. SSH via Tailscale only (ssh stefan@toucan, ssh stefan@hornbill). Network: Cloudflare Zero Trust tunnels → *.sgl.as → servers connected via Tailscale mesh. Security: public access Cloudflare Tunnel only, internal Tailscale, UFW denies all except tailscale0 and port 443. Database: each app owns Postgres instance, cross-app analytics via read-only DB synced nightly. Directories: Hornbill /srv/apps/sgos-<name>/ contains app.json, docker-compose.yml, .env, src/, data/, backup/; /srv/proxy/hornbill/ for maintenance mode; /srv/services/alloy/ for logs. Toucan /srv/config/monitoring/ for Grafana/Loki/Alloy config; /srv/services/ contains status/, webhook/, backups/, glitchtip/, sgos-infra/; /srv/backups/ has staging/ and status.json; /srv/proxy/toucan/ for maintenance mode.

---
## Deployment
path: docs/infrastructure/deployment.md
url: https://sgos-infra.sgl.as/infrastructure/deployment

Deployment model: source-based (code on server via git), Docker Compose orchestration, Cloudflare Tunnel external access, GitHub webhooks automation, app.json + docker-compose.yml config, SOPS-encrypted .env.sops secrets. Branch strategy: main = production (auto-deploys on push), feature branches = development (no auto-deploy). Webhook architecture: GitHub push → Toucan webhook:9000 → hooks.json → deploy-*.sh → SSH to target → git pull, docker build, health check. Webhook security: HMAC-SHA256 signature validation, branch filter (refs/heads/main only), repository filter (sonnenglas/* repos). Current hooks: deploy-sgos-infra (sonnenglas/sgos-infra → Toucan, deploy-sgos-infra.sh), deploy-sgos-phone (sonnenglas/sgos-phone → Hornbill, deploy-sgos-phone.sh), deploy-sgos-sangoma (sonnenglas/sgos-sangoma → Toucan, deploy-sgos-sangoma.sh). Maintenance mode: single nginx reverse proxy per server (sgos-proxy) between Cloudflare Tunnel and apps, checks flag file existence → serves maintenance.html or forwards to app. Maintenance flow: touch flag → nginx serves maintenance page → app rebuilds → health check passes → remove flag → nginx resumes. Manual maintenance: touch /srv/proxy/hornbill/flags/<app>.flag (enter), rm flag (exit), touch global.flag (all apps). New app setup: 1) create webhook/scripts/deploy-sgos-<app>.sh (SSH to target, touch PROXY_FLAG, git pull, sops decrypt, docker compose up --build, wait health check, rm flag), 2) add hook to hooks.json with HMAC-SHA256 trigger rule, 3) GitHub webhook: payload URL https://webhook.sgl.as/hooks/deploy-sgos-<app>, content-type application/json, secret = WEBHOOK_SECRET, push event only, 4) add nginx server block to /srv/proxy/hornbill/nginx.conf with resolver 127.0.0.11, maintenance error_page 503, flag checks, 5) restart webhook on Toucan (docker compose down/up), reload proxy on Hornbill (docker exec sgos-proxy nginx -s reload). Manual deployment: SSH to server, touch flag, cd src && git pull, sops -d src/.env.sops > .env, docker compose up -d --build, rm flag. Rollback: git log --oneline, git checkout <commit>, docker compose up -d --build; check app.json migration field first, if breaking restore database backup. Troubleshooting: webhook logs (docker logs -f webhook on Toucan), stuck maintenance (rm flag), 502 after restart (wait 10s DNS cache or nginx -s reload), app not reachable (verify sgos network: docker inspect/network connect). Files: /srv/services/webhook/hooks.json (hook definitions), /srv/services/webhook/scripts/ (deploy scripts), /srv/proxy/*/nginx.conf (proxy config), /srv/proxy/*/flags/ (maintenance flags).

---
## Monitoring
path: docs/infrastructure/monitoring.md
url: https://sgos-infra.sgl.as/infrastructure/monitoring

Monitoring services: SGOS Status (sgos-status.sgl.as, Toucan:3004, real-time health dashboard), Grafana (grafana.sgl.as, log analytics), GlitchTip (glitchtip.sgl.as, Toucan:8000 internal, Sentry-compatible error tracking). Log aggregation: Alloy collects Docker container logs on both servers (labeled server=toucan/server=hornbill), sends to Loki on Toucan:3100 (30-day retention), Grafana queries. LogQL examples: {server="hornbill"}, {container="phone"}, {server="toucan"} |= "error". Status page architecture: status.py on Toucan SSHs to Hornbill, collects docker inspect/git log/app.json, writes apps.json, served via nginx:80 through Cloudflare Tunnel. Status states: healthy (green pulse), unhealthy (red pulse), starting (yellow pulse), not-running (gray). Scheduled jobs: Toucan backup-orchestrator.sh daily 03:00, status.py every minute. Auto-login via Cloudflare Zero Trust: Cf-Access-Authenticated-User-Email header; GlitchTip uses Django REMOTE_USER; Beszel uses TRUSTED_AUTH_HEADER; Dozzle uses DOZZLE_AUTH_PROVIDER=forward-proxy with DOZZLE_AUTH_HEADER_USER/EMAIL. Watchtower auto-updates monitoring containers daily 4 AM (requires label com.centurylinklabs.watchtower.enable=true). Paths: Grafana/Loki/Alloy at Toucan /srv/config/monitoring/, GlitchTip at Toucan /srv/services/glitchtip/, Status at Toucan /srv/services/status/, Alloy at Hornbill /srv/services/alloy/. SSH deploy key: /home/stefan/.ssh/deploy_hornbill. Health check command: docker inspect --format='{{.State.Health.Status}}' sgos-<app>-app.

---
## Backups
path: docs/infrastructure/backups.md
url: https://sgos-infra.sgl.as/infrastructure/backups

Backups: Two-stage architecture - apps create local backups, Toucan orchestrates collection and offsite sync. Flow: Hornbill /srv/apps/*/backups/ (app outputs, 7-day local retention) → rsync pull 3 AM → Toucan /srv/backups/staging/ (latest sync only) → restic encrypted → Cloudflare R2 sonnenglas-backups (7 daily, 4 weekly, 3 monthly retention). Orchestrator: /srv/services/backups/backup-orchestrator.sh runs nightly 3 AM cron on Toucan, SSHs to servers, executes app backup.sh scripts, pulls via rsync, syncs to R2. Files on Toucan: /srv/services/backups/.env (R2 credentials, restic password), /srv/services/backups/backup-orchestrator.sh, /srv/backups/staging/, /srv/backups/status.json (per-app status), /srv/backups/backup.log. Backed up: sgos-phone (PostgreSQL database, voicemail MP3s). Not backed up: Redis (cache), Loki logs (30-day internal retention), Docker images (registry-pulled), source code (GitHub). Operations: check status `cat /srv/backups/status.json`, view log `tail -100 /srv/backups/backup.log`, list R2 snapshots `cd /srv/services/backups && source .env && export AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY RESTIC_REPOSITORY RESTIC_PASSWORD && restic snapshots`, manual backup `/srv/services/backups/backup-orchestrator.sh`. Restore: setup exports (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, RESTIC_REPOSITORY, RESTIC_PASSWORD from .env), `restic snapshots` to list, `restic ls latest` to browse, `restic restore latest --target /tmp/restore --include "sgos-phone"` to restore. Phone DB restore: restic restore to /tmp, scp database.sql to hornbill:/tmp, on Hornbill stop phone container, `docker exec -i sgos-phone-db psql -U postgres -d phone < /tmp/database.sql`, restart. Voicemail restore: `restic restore latest --target /tmp/restore --include "sgos-phone/voicemails"`, scp MP3s to hornbill:/srv/apps/sgos-phone/data/voicemails/. Specific date: use snapshot ID from `restic snapshots`. Verification: `restic check` for integrity, `restic restore latest --target /tmp/verify-test --verify`. Add new app: create backup.sh in app repo, add scripts.backup and sgos.backup.output to app.json, add app to backup-orchestrator.sh on Toucan.

---
## Secrets
path: docs/infrastructure/secrets.md
url: https://sgos-infra.sgl.as/infrastructure/secrets

Secrets encrypted with SOPS + age encryption. File convention: .env.sops (encrypted, committed), .env (decrypted, gitignored), .env.example (template, optional). Config file .sops.yaml at repo root with creation_rules path_regex \.env\.sops$ and age public key age1nh9zuzsmewquyr0xlv7vzzsug0fat6ju5kznxhlpkcrujwtjevyqe6vl5g. Private key location on all machines (Mac, Toucan, Hornbill): ~/.config/sops/age/keys.txt. Environment variable: export SOPS_AGE_KEY_FILE="$HOME/.config/sops/age/keys.txt". Operations: create secret (echo to .env, sops -e -i, rename to .env.sops), edit (sops service/.env.sops opens in $EDITOR), decrypt for deploy (sops --input-type dotenv --output-type dotenv -d .env.sops > .env), view (sops -d .env.sops). Deployment workflow: git pull, decrypt with sops, docker compose up -d. Secret rotation: edit locally with sops, commit/push, on server git pull && sops -d .env.sops > .env && docker compose restart. Security: private key in 1Password and servers, public key safe to commit, same age key works across all repos. Backup: 1Password (primary), server key files. Key loss = unrecoverable secrets.

---
## Cloudflare
path: docs/infrastructure/cloudflare.md
url: https://sgos-infra.sgl.as/infrastructure/cloudflare

Cloudflare managed via Terraform in /cloudflare/. Resources: tunnels (toucan.sgl.as, hornbill), tunnel configs (ingress rules), DNS CNAME records, Access Applications (Zero Trust), Access Policies. Workflow: cd cloudflare && terraform plan && terraform apply. Adding service: 1) ingress rule in tunnel.tf, 2) DNS CNAME in dns.tf, 3) optional Zero Trust in access.tf, 4) terraform apply. Secrets: terraform.tfvars (gitignored) with API token requiring Zone/DNS/Edit, Account/Zero Trust/Edit, Account/Cloudflare Tunnel/Edit. State: terraform.tfstate (local, gitignored, backup required). cloudflared daemon runs on Toucan and Hornbill as systemd service with token-based config: ExecStart=/usr/bin/cloudflared --no-autoupdate tunnel run --token <token>. Ingress rules managed remotely via Terraform cloudflare_zero_trust_tunnel_cloudflared_config, no local config files, changes require terraform apply. Apps use nginx sidecar for maintenance mode. Status check: systemctl status cloudflared, journalctl -u cloudflared -f. Zero Trust auth methods: Google Workspace (primary team login), PocketID OIDC (non-Google users), Service Tokens (API/automation). Access app HCL: cloudflare_zero_trust_access_application with zone_id, name, domain, type=self_hosted, session_duration=24h; cloudflare_zero_trust_access_policy with application_id, zone_id, name, decision=allow, precedence, include gsuite block with identity_provider_id.

---
## Disaster Recovery
path: docs/infrastructure/disaster-recovery.md
url: https://sgos-infra.sgl.as/infrastructure/disaster-recovery

Disaster Recovery scenarios: (1) Hornbill failure impacts all SGOS apps, recovery: provision new server (Ubuntu 24.04 LTS, min 8 vCPU/16GB RAM/500GB NVMe), install Tailscale+Docker, restore SSH keys from Toucan (scp stefan@toucan:/home/stefan/.ssh/deploy_hornbill), create /srv/{infra,services,apps}, restore apps via restic from s3:s3.eu-central-1.amazonaws.com/sgos-backups, clone repos, decrypt secrets (sops -d src/.env.sops > .env), update Cloudflare tunnel IP via terraform apply in /cloudflare, RTO ~2h RPO ~24h. (2) Toucan failure impacts monitoring/backups/webhooks/status, apps continue, recovery: provision server (Ubuntu 24.04 LTS, min 4 vCPU/8GB RAM/200GB NVMe), restore monitoring stack to /srv/services/monitoring, restore backup orchestrator to /srv/services/backups, restore crons (0 3 * * * /srv/services/backups/backup-orchestrator.sh, * * * * * /usr/bin/python3 /srv/services/status/status.py), generate new SSH key (ssh-keygen -t ed25519 -f ~/.ssh/deploy_hornbill), restore cloudflared tunnel with token, restore R2 credentials from 1Password, RTO ~3h. (3) Data corruption: stop app, list snapshots via restic, restore specific snapshot with --include filter, replace data, restore database via docker exec psql. (4) SOPS key loss: retrieve from 1Password (SGOS vault → "age secret key") to ~/.config/sops/age/keys.txt, if unavailable regenerate all secrets and rotate credentials. (5) Cloudflare compromise: revoke API tokens, rotate tunnel secrets, review Zero Trust logs, generate new Terraform token, re-run terraform apply. Not backed up (recreate from docs): server OS config (hornbill-setup.md, toucan-setup.md), Docker networks, Cloudflare tunnels (Terraform), SSH keys, cron jobs, Grafana dashboards. Emergency contacts: Netcup (support@netcup.de), Cloudflare (dashboard), GitHub, 1Password. Quarterly DR tests: backup restore to /tmp, key recovery via 1Password, documentation walkthrough on test VM.

---
## Authentication
path: docs/infrastructure/authentication.md
url: https://sgos-infra.sgl.as/infrastructure/authentication

Authentication: All services behind Cloudflare Zero Trust. Methods: Google Workspace (primary IdP, @sonnenglas.net), PocketID (self-hosted OIDC, passkey auth, https://id.sgl.as), Cloudflare header auto-login (Cf-Access-Authenticated-User-Email), App OIDC. Apps: Dashboard (dashboard.sgl.as, Zero Trust Google, no login), Beszel (beszel.sgl.as, Zero Trust Google, auto-login via TRUSTED_AUTH_HEADER=Cf-Access-Authenticated-User-Email), Dozzle (dozzle.sgl.as, Zero Trust Google, auto-login via DOZZLE_AUTH_PROVIDER=forward-proxy, DOZZLE_AUTH_HEADER_USER/EMAIL/NAME=Cf-Access-Authenticated-User-Email), SGOS Infra Docs (sgos-infra.sgl.as, Zero Trust Google, static), GlitchTip (glitchtip.sgl.as, Zero Trust Google, requires separate OIDC login via PocketID/Google, configure in Django Admin /admin/socialaccount/socialapp/ with provider OpenID Connect, settings {"server_url": "https://id.sgl.as"}), PocketID (id.sgl.as, no Zero Trust, passkey login, is the IdP, env: APP_URL=https://id.sgl.as, TRUST_PROXY=true, ENCRYPTION_KEY), Phone (phone.sgl.as, no Zero Trust, public webhook). PocketID OIDC endpoints: authorize https://id.sgl.as/authorize, token https://id.sgl.as/api/oidc/token, JWKS https://id.sgl.as/.well-known/jwks.json. Cloudflare Zero Trust PocketID integration: Settings → Authentication → Add OpenID Connect login method.
